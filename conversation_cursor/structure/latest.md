# Structure: Project Organization

## Project Structure Overview

```
job-seeking-webpage/
â”œâ”€â”€ read_it.md                      # Project guidelines (read first)
â”œâ”€â”€ pyproject.toml                  # Poetry configuration
â”œâ”€â”€ README.md                       # Project documentation
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                        # Raw scraped HTML/XML (Phase 1 output)
â”‚   â”‚   â”œâ”€â”€ aea/                    # AEA JOE scraped files
â”‚   â”‚   â”œâ”€â”€ universities/           # University scraped files
â”‚   â”‚   â””â”€â”€ institutes/             # Research institute scraped files
â”‚   â”œâ”€â”€ processed/                  # Processed data (Phase 2 output)
â”‚   â”‚   â”œâ”€â”€ jobs.json               # Current listings
â”‚   â”‚   â”œâ”€â”€ jobs.csv                # Current listings (CSV)
â”‚   â”‚   â”œâ”€â”€ archive/                # Historical snapshots
â”‚   â”‚   â””â”€â”€ diagnostics/            # Diagnostic reports
â”‚   â””â”€â”€ config/
â”‚       â”œâ”€â”€ scraping_sources.json   # Scraping sources (176 accessible URLs)
â”‚       â”œâ”€â”€ scraping_rules.json     # Scraping patterns
â”‚       â””â”€â”€ processing_rules.json   # Processing rules (job types, specializations, regions)
â”‚
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ scraper/                    # Phase 1 - COMPLETE
â”‚   â”‚   â”œâ”€â”€ base_scraper.py         # Base scraper class
â”‚   â”‚   â”œâ”€â”€ aea_scraper.py          # AEA JOE scraper
â”‚   â”‚   â”œâ”€â”€ university_scraper.py   # University scraper
â”‚   â”‚   â”œâ”€â”€ institute_scraper.py    # Institute scraper
â”‚   â”‚   â”œâ”€â”€ parsers/                # HTML, RSS, text, date parsers
â”‚   â”‚   â””â”€â”€ utils/                  # Rate limiter, retry handler, user agent
â”‚   â”œâ”€â”€ processor/                  # Phase 2 - IN PROGRESS
â”‚   â”‚   â”œâ”€â”€ pipeline.py            # Main pipeline (âœ… Phase 2A)
â”‚   â”‚   â”œâ”€â”€ parser_manager.py       # Route to parsers (âœ… Phase 2A)
â”‚   â”‚   â”œâ”€â”€ normalizer.py           # Data normalization (âœ… Phase 2A - basic)
â”‚   â”‚   â”œâ”€â”€ enricher.py             # Data enrichment (to be created)
â”‚   â”‚   â”œâ”€â”€ deduplicator.py         # Deduplication (to be created)
â”‚   â”‚   â”œâ”€â”€ validator.py            # Data validation (to be created)
â”‚   â”‚   â”œâ”€â”€ schema.py               # Schema definition (âœ… created)
â”‚   â”‚   â”œâ”€â”€ diagnostics.py          # Diagnostic tracking (âœ… Phase 2A)
â”‚   â”‚   â””â”€â”€ utils/                  # ID generator (âœ…), location parser, text cleaner (âœ…)
â”‚   â”œâ”€â”€ generator/                  # Phase 3 - PENDING
â”‚   â””â”€â”€ scheduler.py                # Phase 4 - PENDING
â”‚
â”œâ”€â”€ tests/                          # Tests organized by phase
â”‚   â”œâ”€â”€ setup-project/              # Phase 0 tests
â”‚   â”œâ”€â”€ load-data-collection/       # Phase 1 tests
â”‚   â”œâ”€â”€ transform-data-processing/  # Phase 2 tests (âœ… Phase 2A component tests)
â”‚   â””â”€â”€ export-output-generation/   # Phase 3 tests (to be created)
â”‚
â”œâ”€â”€ templates/                      # HTML templates (Phase 3)
â”œâ”€â”€ static/                         # CSS, JS, images (Phase 3)
â”‚
â””â”€â”€ conversation_cursor/            # Project management
    â”œâ”€â”€ dates/                      # Dated proposals
    â”œâ”€â”€ progress/latest.md         # High-level pipeline status
    â”œâ”€â”€ structure/latest.md         # This file - project structure
    â””â”€â”€ to-do-list/                 # Detailed task lists
```

## Key Files & Modules

### Configuration
- **`data/config/scraping_sources.json`**: 176 accessible URLs across job portals, universities, research institutes
- **`data/config/scraping_rules.json`**: Scraping patterns (deadlines, keywords, date formats)
- **`data/config/processing_rules.json`**: Processing rules (job type keywords, specialization keywords, region mapping, materials parsing)

### Scraper Framework (Phase 1 - Complete)
- **`scripts/scraper/base_scraper.py`**: Abstract base class (fetch, parse, extract, save)
- **`scripts/scraper/aea_scraper.py`**: AEA JOE scraper (RSS/HTML fallback)
- **`scripts/scraper/university_scraper.py`**: Generic university scraper (pattern-based)
- **`scripts/scraper/institute_scraper.py`**: Research institute scraper
- **`scripts/scraper/parsers/`**: HTML, RSS, text extractor, date parser
- **`scripts/scraper/utils/`**: Rate limiter, retry handler, user agent, config loader

### Processor Framework (Phase 2 - In Progress)
- **`scripts/processor/pipeline.py`**: Main processing pipeline orchestrator
- **`scripts/processor/parser_manager.py`**: Routes raw data to parsers
- **`scripts/processor/normalizer.py`**: Normalizes dates, locations, formats, text
- **`scripts/processor/enricher.py`**: Enriches data (IDs, classifications, metadata)
- **`scripts/processor/deduplicator.py`**: Identifies and merges duplicate listings
- **`scripts/processor/validator.py`**: Validates data against schema (to be created)
- **`scripts/processor/diagnostics.py`**: Diagnostic tracking and root cause analysis âœ… Phase 2A
- **`scripts/processor/schema.py`**: Data schema definition (29 fields, validation functions) âœ…
- **`scripts/processor/utils/`**: ID generator âœ…, location parser (to be created), text cleaner âœ…
- **`scripts/processor/pipeline.py`**: Main processing pipeline orchestrator âœ… Phase 2A
- **`scripts/processor/parser_manager.py`**: Routes raw data to parsers âœ… Phase 2A
- **`scripts/processor/normalizer.py`**: Basic data normalization âœ… Phase 2A

## Workflow

**Load â†’ Transform â†’ Export**
1. **LOAD (Phase 1 - âœ…)**: Scrape raw HTML from sources
2. **TRANSFORM (Phase 2 - ğŸš€)**: Process, normalize, deduplicate, validate data
3. **EXPORT (Phase 3 - â¸ï¸)**: Generate HTML/JSON/CSV outputs
4. **DEPLOY (Phase 4 - â¸ï¸)**: Automation and hosting

## Environment

- **Python**: 3.13.5 (via Poetry)
- **Virtual Environment**: `./environment/python/venv/`
- **Run**: `poetry run python scripts/scraper/aea_scraper.py`
