# To-Do List: 2025-12-31

## Completed Tasks âœ…

1. âœ… Created `.cursorrules` file to enforce reading `read_it.md` before any operation
2. âœ… Designed comprehensive proposal for Economics Faculty Job Aggregator project
3. âœ… Created workflow illustration and folder structure design documents
4. âœ… Set up complete folder structure (data/, scripts/, templates/, static/)
5. âœ… Created initial configuration files (universities.json, scraping_rules.json)
6. âœ… Created README.md and requirements.txt
7. âœ… Updated documentation: raw data stores only latest version (no daily folders)
8. âœ… Set up Poetry virtual environment and installed dependencies
9. âœ… Updated read_it.md and .cursorrules (removed duplicates, added commit task, explicit reading instructions)
10. âœ… Created dated to-do list file (2025-12-31_task.md)

## In Progress ðŸ”„

- None currently

## Pending Tasks (In Order)

### Phase 1: Foundation
1. Build base scraper framework (`scripts/scraper/base_scraper.py`)
2. Implement AEA JOE scraper (`scripts/scraper/aea_scraper.py`)
3. Test AEA scraper with sample data
4. Create data storage structure validation

### Phase 2: Data Processing
5. Build parser for extracted data (`scripts/processor/parser.py`)
6. Implement date normalization (`scripts/processor/normalizer.py`)
7. Create deduplication logic (`scripts/processor/deduplicator.py`)
8. Design and implement data schema (JSON structure)
9. Test data processing pipeline end-to-end

### Phase 3: Output Generation
10. Create HTML template for job listings (`templates/jobs_template.html`)
11. Build HTML generator (`scripts/generator/html_generator.py`)
12. Implement JSON/CSV export (`scripts/generator/json_generator.py`)
13. Add basic styling (`static/css/styles.css`)
14. Test output generation with sample data

### Phase 4: University Scrapers
15. Create generic university scraper (`scripts/scraper/university_scraper.py`)
16. Add top 20 economics departments to config
17. Handle different website structures
18. Test and refine scraping rules for each university
19. Validate scraping accuracy

### Phase 5: Automation & Polish
20. Set up daily scheduler (`scripts/scheduler.py`)
21. Add error handling and logging
22. Implement search/filter functionality (`static/js/filter.js`)
23. Add metadata (last updated timestamp)
24. Create comprehensive README with setup instructions
25. Test full automation cycle

### Phase 6: Enhancement (Future)
26. Add more universities to scraping list
27. Improve parsing accuracy based on real data
28. Add email notifications for new postings
29. Create API endpoint for programmatic access
30. Add analytics/tracking

## Notes

- All tasks should follow the `load â†’ transform â†’ export` workflow structure
- Commit changes as a separate, explicit task after completing work
- Update progress and structure files after each major milestone

